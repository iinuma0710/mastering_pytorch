# PyTorch を用いた深層学習の概要
深層学習は，計算機や機械を用いた実世界での認識タスクにおいて，革命を起こしてきた機械学習の一種です．
DNN (Deep Neural Network) は数学的な概念に基づいて，膨大なデータな中から入力と出力の間にある非自明な関係を，複雑な非線形関数の形で学習しています．

DNN は膨大な数学的演算と線型代数方程式，複雑な非線形関数，そして様々な最適化アルゴリズムから成り立っています．
Python のようなプログラミング言語を用いて，一から DNN を構築し学習を行うには，必要な方程式や関数，最適化スケジュールを記述必要があります
さらに，膨大なデータを効率的に読み込み，適切な時間で学習できるようにコードを書かなければなりません
これを DNN のアプリケーションを構築するたびに自分でやろうとすると，いくらかより低レベルの実装が必要となってしまいます．

このような手間を省くため，Theano や TensorFlow といった深層学習ライブラリが開発されてきました．
PyTorch も Python ベースの深層学習ライブラリの一つです．

TensorFlow は Python (および C++) のオープンソースライブラリとして，2015年末に Google に発表されました．
これにより，深層学習の適用分野が大きく広がりました．
これに対して，2016年には Facebook が同じくオープンソースとして Torch と呼ばれるライブラリを発表しました．
Torch は当初 Lua というスクリプト言語に向けたものでしたが，すぐに Python でも同等のライブラリである PyTorch が誕生しました．
時を同じくして Microsoft は CNTK と呼ばれる独自のライブラリを発表しています．
こうした競争の中で，PyTorch は最もよく使われる深層学習ライブラリの1つとして急成長しました．

本書では，複雑な深層学習アーキテクチャを用いてどのように最先端の課題を解決し，そのモデルを PyTorch ではどのように構築，学習し，評価するのかをハンズオン形式で伝えていきます．
あくまで PyTorch に軸足を置きつつ，近年の深層学習モデルについても取り扱います．
本書は，データサイエンティストや機械学習エンジニア，あるいは Python (できれば PyTorch) での実務経験のある研究者を対象としています．

ハンズオンの性質上，PyTorch の記述に慣れるためにも，各章のサンプルコードを自分で書いて実行してみることを強くおすすめします．
PyTorch の導入から初めて，章を追うごとに様々な深層学習の課題とモデルアーキテクチャを見ていきます．

この章では，深層学習の背景にある概念や PyTorch の概要を説明します．
章末では，練習として PyTorch でモデルの学習を行います．

## 深層学習の概要
ニューラルネットワークは人間の脳の構造と機能に着想を得た機械学習の一種です．
ニューラルネットワークでは，各計算ユニットを人間の脳を模してニューロンと呼び，層状に他のニューロンと結合しています．
この層の数が２よりも多いニューラルネットワークを Deep Neural Network (DNN) と呼ぶのです．
このようなモデルは，一般的に深層学習モデルと呼ばれます．

深層学習モデルは，従来の機械学習モデルに比べて入力と出力のとても複雑な関係性を学習する能力が優れています．
最近では，

- 特にクラウドにおいて，計算能力の高いマシンが使えるようになったこと
- 膨大なデータが入手できるようになったこと

などの要因から，深層学習に注目が集まっています．
２年でコンピュータの計算能力が２倍になるというムーアの法則によって，数百層にも及ぶ深層学習モデルを現実的かつ実用的な短時間で学習できるようになりました．
それと同時に，爆発的にデジタルデバイスが広く使われるようになり，常に膨大なデータが世界中で生成されるようになっています．
これによって，従来の機械学習手法では扱うことすら難しかったり，次善の方法でしか解けなかったりした難しい認識タスクを学習することができるようになったのです．

深層学習にはもう一つ，従来の機械学習手法より優れた特徴があります．
古典的な機械学習に基づくアプローチでは，しばしば特徴量エンジニアリングが，モデルの学習にとって非常に重要な役割を果たしてきました．
しかし，深層学習では手動で特徴量を作成する必要はありません．
膨大なデータを用いて特徴量を手動設計することなく学習を行い，従来のモデルの性能を超えることができるのです．

深層学習モデルは何年もかけて開発されてきた様々なタイプのニューラルネットワークのアーキテクチャをもとに構築することが可能です．
アーキテクチャ間の第一の違いは，ニューラルネットワークで使われているレイヤの種類と組み合わせです．
よく知られているレイヤには次のようなものがあります．

### 全結合層 (Fully-connected layer) あるいは線形層 (Linear layer)
全結合層のあるニューロンは前後の層の全てのニューロンと結合されています．
この全結合層は，多くの深層学習による分類器において，基礎的な構成要素となっています．

<div align="center">
    <image src="../../images/fcn.png" />
</div>

### 畳み込み層 (Convolutional layer)
畳み込み層は CNN (Convolutional Neural Networks) の構成要素で，コンピュータビジョン系の課題解決に最も効果的なモデルです．

<div align="center">
    <image src="../../images/conv.png" />
</div>

### 回帰 (Recurrent)
一見すると全結合層と変わりませんが，回帰接続 (recurrent connection) を持っています．
全結合層とは異なり現在の値を記憶しておくことが可能で，現在の入力に対して過去の入力を記憶しておく必要のある，逐次的なデータ処理で用いられます．

<div align="center">
    <image src="../../images/rnn.png" />
</div>

### 逆畳み込み (Deconvolution)
畳み込みとは逆の作用を持つレイヤで，入力を空間的に拡張する働きがあります．
そのため，画像の生成や再構築などを行うモデルで特に重要になってきます．

<div align="center">
    <image src="../../images/deconv.png" />
</div>

### プーリング (Pooling)
最大値を取る Max-pooling や最小値を取る Min-pooling，平均値を取る Mean (Average) -pooling があります．

<div align="center">
    <image src="../../images/pooling.png" />
</div>

### ドロップアウト (Dropout)
ドロップアウトでは，一部のニューロンを一時的にオフの状態，すなわちネットワークから切り離された状態にします．
ドロップアウトすることによってモデルが正則化することができます．
これは，特定のニューロンが散発的に欠落してもモデルがうまく機能するように強制するもので，これによりモデルは学習データ全体を記憶する代わりに，一般化可能なパターンを学習することになります．

<div align="center">
    <image src="../../images/dropout.png" />
</div>

これらの層を組み合わせて作られるアーキテクチャには，次の図のようなものがあります．

<div align="center">
    <image src="../../images/architectures.png" />
</div>

## 活性化関数 (Activation Function)
線形層をどれだけ重ねたとしても，単一の線形モデルに集約できてしまいます．
そこで活性化関数によってネットワークに非線形性を与えることが重要になってきます．
活性化関数には次のような種類があります．

### シグモイド (Sigmoid) 関数
シグモイド (あるいはロジスティック (logistic)) 関数は次の式で与えられます．
この関数では，入力の値を (0, 1) の範囲に変換します．

$$ y=f(x)=\frac{1}{1+e^{-x}} $$

<div align="center">
    <image src="../../images/sigmoid.png" />
</div>


### tanh 関数 (ハイパボリックタンジェント)
シグモイドとは異なり (-1, 1) の値を取る関数が tanh 関数です．
正の出力だけでなく負の出力が必要な場合に使われます．

$$ y=f(x)=\frac{e^x-e^{-x}}{e^x+e^{-x}} $$

<div align="center">
    <image src="../../images/tanh.png" />
</div>


### Rectified linear units (ReLUs) 関数
上記の活性化関数よりも新しくシンプルなものとなっています．

$$ y = f(x) = max(0,x) $$

<div align="center">
    <image src="../../images/relu.png" />
</div>