# PyTorch を用いた深層学習の概要
深層学習は，計算機や機械を用いた実世界での認識タスクにおいて，革命を起こしてきた機械学習の一種です．
DNN (Deep Neural Network) は数学的な概念に基づいて，膨大なデータな中から入力と出力の間にある非自明な関係を，複雑な非線形関数の形で学習しています．

DNN は膨大な数学的演算と線型代数方程式，複雑な非線形関数，そして様々な最適化アルゴリズムから成り立っています．
Python のようなプログラミング言語を用いて，一から DNN を構築し学習を行うには，必要な方程式や関数，最適化スケジュールを記述必要があります
さらに，膨大なデータを効率的に読み込み，適切な時間で学習できるようにコードを書かなければなりません
これを DNN のアプリケーションを構築するたびに自分でやろうとすると，いくらかより低レベルの実装が必要となってしまいます．

このような手間を省くため，Theano や TensorFlow といった深層学習ライブラリが開発されてきました．
PyTorch も Python ベースの深層学習ライブラリの一つです．

TensorFlow は Python (および C++) のオープンソースライブラリとして，2015年末に Google に発表されました．
これにより，深層学習の適用分野が大きく広がりました．
これに対して，2016年には Facebook が同じくオープンソースとして Torch と呼ばれるライブラリを発表しました．
Torch は当初 Lua というスクリプト言語に向けたものでしたが，すぐに Python でも同等のライブラリである PyTorch が誕生しました．
時を同じくして Microsoft は CNTK と呼ばれる独自のライブラリを発表しています．
こうした競争の中で，PyTorch は最もよく使われる深層学習ライブラリの1つとして急成長しました．

本書では，複雑な深層学習アーキテクチャを用いてどのように最先端の課題を解決し，そのモデルを PyTorch ではどのように構築，学習し，評価するのかをハンズオン形式で伝えていきます．
あくまで PyTorch に軸足を置きつつ，近年の深層学習モデルについても取り扱います．
本書は，データサイエンティストや機械学習エンジニア，あるいは Python (できれば PyTorch) での実務経験のある研究者を対象としています．

ハンズオンの性質上，PyTorch の記述に慣れるためにも，各章のサンプルコードを自分で書いて実行してみることを強くおすすめします．
PyTorch の導入から初めて，章を追うごとに様々な深層学習の課題とモデルアーキテクチャを見ていきます．

この章では，深層学習の背景にある概念や PyTorch の概要を説明します．
章末では，練習として PyTorch でモデルの学習を行います．

## 深層学習の概要
ニューラルネットワークは人間の脳の構造と機能に着想を得た機械学習の一種です．
ニューラルネットワークでは，各計算ユニットを人間の脳を模してニューロンと呼び，層状に他のニューロンと結合しています．
この層の数が２よりも多いニューラルネットワークを Deep Neural Network (DNN) と呼ぶのです．
このようなモデルは，一般的に深層学習モデルと呼ばれます．

深層学習モデルは，従来の機械学習モデルに比べて入力と出力のとても複雑な関係性を学習する能力が優れています．
最近では，

- 特にクラウドにおいて，計算能力の高いマシンが使えるようになったこと
- 膨大なデータが入手できるようになったこと

などの要因から，深層学習に注目が集まっています．
２年でコンピュータの計算能力が２倍になるというムーアの法則によって，数百層にも及ぶ深層学習モデルを現実的かつ実用的な短時間で学習できるようになりました．
それと同時に，爆発的にデジタルデバイスが広く使われるようになり，常に膨大なデータが世界中で生成されるようになっています．
これによって，従来の機械学習手法では扱うことすら難しかったり，次善の方法でしか解けなかったりした難しい認識タスクを学習することができるようになったのです．

深層学習にはもう一つ，従来の機械学習手法より優れた特徴があります．
古典的な機械学習に基づくアプローチでは，しばしば特徴量エンジニアリングが，モデルの学習にとって非常に重要な役割を果たしてきました．
しかし，深層学習では手動で特徴量を作成する必要はありません．
膨大なデータを用いて特徴量を手動設計することなく学習を行い，従来のモデルの性能を超えることができるのです．

深層学習モデルは何年もかけて開発されてきた様々なタイプのニューラルネットワークのアーキテクチャをもとに構築することが可能です．
アーキテクチャ間の第一の違いは，ニューラルネットワークで使われているレイヤの種類と組み合わせです．
よく知られているレイヤには次のようなものがあります．

### 全結合層 (Fully-connected layer) あるいは線形層 (Linear layer)
全結合層のあるニューロンは前後の層の全てのニューロンと結合されています．
この全結合層は，多くの深層学習による分類器において，基礎的な構成要素となっています．

<div align="center">
    <image src="../../images/fcn.png" width="320" />
</div>

### 畳み込み層 (Convolutional layer)
畳み込み層は CNN (Convolutional Neural Networks) の構成要素で，コンピュータビジョン系の課題解決に最も効果的なモデルです．

<div align="center">
    <image src="../../images/conv.png" width="640" />
</div>

### 回帰 (Recurrent)
一見すると全結合層と変わりませんが，回帰接続 (recurrent connection) を持っています．
全結合層とは異なり現在の値を記憶しておくことが可能で，現在の入力に対して過去の入力を記憶しておく必要のある，逐次的なデータ処理で用いられます．

<div align="center">
    <image src="../../images/rnn.png" width="320" />
</div>

### 逆畳み込み (Deconvolution)
畳み込みとは逆の作用を持つレイヤで，入力を空間的に拡張する働きがあります．
そのため，画像の生成や再構築などを行うモデルで特に重要になってきます．

<div align="center">
    <image src="../../images/deconv.png" width="640" />
</div>

### プーリング (Pooling)
最大値を取る Max-pooling や最小値を取る Min-pooling，平均値を取る Mean (Average) -pooling があります．

<div align="center">
    <image src="../../images/pooling.png" width="640" />
</div>

### ドロップアウト (Dropout)
ドロップアウトでは，一部のニューロンを一時的にオフの状態，すなわちネットワークから切り離された状態にします．
ドロップアウトすることによってモデルが正則化することができます．
これは，特定のニューロンが散発的に欠落してもモデルがうまく機能するように強制するもので，これによりモデルは学習データ全体を記憶する代わりに，一般化可能なパターンを学習することになります．

<div align="center">
    <image src="../../images/dropout.png" width="320" />
</div>

これらの層を組み合わせて作られるアーキテクチャには，次の図のようなものがあります．

<div align="center">
    <image src="../../images/architectures.png" width="320" />
</div>

## 活性化関数 (Activation Function)
線形層をどれだけ重ねたとしても，単一の線形モデルに集約できてしまいます．
そこで活性化関数によってネットワークに非線形性を与えることが重要になってきます．
活性化関数には次のような種類があります．

### シグモイド (Sigmoid) 関数
シグモイド (あるいはロジスティック (logistic)) 関数は次の式で与えられます．
この関数では，入力の値を (0, 1) の範囲に変換します．

$$
y=f(x)=\frac{1}{1+e^{-x}}
$$

<div align="center">
    <image src="../../images/sigmoid.png" width="320" />
</div>


### tanh 関数 (ハイパボリックタンジェント)
シグモイドとは異なり (-1, 1) の値を取る関数が tanh 関数です．
正の出力だけでなく負の出力が必要な場合に使われます．

$$
y=f(x)=\frac{e^x-e^{-x}}{e^x+e^{-x}}
$$

<div align="center">
    <image src="../../images/tanh.png" width="320" />
</div>


### Rectified linear units (ReLUs) 関数
上記の活性化関数よりも新しくシンプルなものとなっています．
シグモイド関数や tanh 関数との違いは、入力が0以上の時には出力が大きくなり続ける点にあります。
これによって勾配が 0 に漸近するのを防ぐことができます。
しかし、入力が負の場合には出力も勾配も常に 0 となります。

$$
y = f(x) = max(0,x)
$$

<div align="center">
    <image src="../../images/relu.png" width="320" />
</div>

### Leaky ReLU
ReLU 関数は負の入力をすべて 0 にしてしまいます。
しかし、負の入力についても処理したい場合があります．
Leaky ReLU は負の入力に対して $k$ をかけた値を出力します．

$$
y = f(x) = max(kx,x)
$$

<div align="center">
    <image src="../../images/leaky_relu.png" width="320" />
</div>

活性化関数は深層学習の分野で活発に研究されている領域です．
ここで，全ての活性化関数を紹介することはできませんが，この分野の最新の動向をチェックしておくことをお勧めします．
多くの活性化関数は，上述の関数を少し修正したものとなっています．

## 最適化スケジュール (Optimization schedule)
ここまでは，ネットワークを構築する方法について見てきました．
ネットワークを学習するには最適化スケジュールを適用する必要があります．
その他のパラメータに基づく機械学習手法と同様に，深層学習モデルもパラメータのチューニングによって学習を行います．
パラメータは誤差逆伝播 (backpropagation) と呼ばれる処理によってチューニングを行い，ネットワークの最終層か出力層から損失 (loss) を計算します．
この損失は，最終層の出力と対応する正解データ (GT: ground truth) から計算します．
その後，勾配降下 (grandient descent) と微分の連鎖律 (chain rule of defferentiation) を用いて，この損失を逆伝播してやるのです．  
各層のパラメータあるいは重みは損失が小さくなるように修正されていきます．
修正の度合いはある係数によって決定します．
この係数は 0 から 1 の範囲で設定し，学習率 (learning rate) と呼ばれます．
このようにニューラルネットワークの重みを更新する過程は，最適化スケジュール (Optimization schedule) と呼ばれ，モデルの学習結果に大きな影響を与えます．
そのため，この分野では多くの研究がなされ，まだなお続いています．

### 確率的勾配降下法 (SGD: Stocastic Gradient Descent)
$\beta$ をパラメータ，$X$ と $y$ をそれぞれ入力の学習データと対応するラベルとして，次式のようにあらわされます．

$$
\beta = \beta - \alpha \times \frac{\delta L(X, y, \beta)}{\delta\beta}
$$

$L$ は損失関数，$\alpha$ は学習率です．
SGD は学習データの各 $(X, y)$ のペアについて更新を行います．
また，ミニバッチ学習の際にはバッチサイズ $k$ ごとに，バッチ学習の際には全データについて一括してパラメータを更新してやります．

### Adagrad
SGD では常に一定の学習率で全てのパラメータを更新していました．
しかし，特にスパースなデータを扱う場合には，各パラメータを異なるペースで更新する必要があるでしょう．
このような場合には，ほかの特徴抽出において特定のパラメータをより積極的に取り入れる必要があります．
Adagrad は次のような式で与えられます．

$$
\beta^{t+1}_{i} = \beta^{t}_{i} - \frac{\alpha}{\sqrt{SSG^{t}_{i}+\epsilon}} \times \frac{\delta L(X, y, \beta)}{\delta\beta^{t}_{i}}
$$

パラメータの添え字は時刻 $t$ における $i$ 番目のパラメータであることを表します．
$SSG^{t}_{i}$ はステップ $0$ からステップ $t$ までの二乗微分の合計 (sum of squared gradient) です．
$\epsilon$ はゼロ除算を防ぐための小さな値です．
$SSG$ の平方根で全体の学習率を割った値を用いて学習することで，頻繁に更新されるパラメータの学習率がめったに更新されないパラメータの学習率よりも早く小さくなることを保証します．

### Adadelta
Adagrad では2乗の累積値で学習率を割っており，その値は学習が進むごとに大きくなっていました．
これによって，学習が進むにつれて学習率がほとんど 0 になってしまいます．
この問題を解決するために，Adadelta では SSG の計算は直前のステップまでの値で計算するようにしました．
実際に，直前の SSG の値と現在の微分値の $\gamma$ による加重平均で表します．

$$
SSG^{t}_{i} = \gamma \times SSG^{t-1}_{i} + (1 - \gamma) \times (\frac{\delta L(X, y, \beta)}{\delta\beta^{t}_{i}})^2
$$

加重平均を導入することで，SSG が大きな値となることを防ぐことができます．
$SSG^{t}_{i}$ が定義されれば，Adagrad と同じように更新処理が可能となります．  
しかし，Adagrad の式をよく見ると，平均2乗勾配の根が無次元量ではないため，学習率の係数として使うには理想的ではないことが分かります．
これを解決するため，平均値をとる別の方法として，ここでは2乗したパラメータを更新してやります．
まずは，パラメータの更新を次のように定義します．

$$
\Delta\beta^{t}_{i} = \beta^{t+1}_{i} - \beta^{t}_{i} = - \frac{\alpha}{\sqrt{SSG^{t}_{i}+\epsilon}} \times \frac{\delta L(X, y, \beta)}{\delta\beta^{t}_{i}}
$$

続いて，SSG の更新式と同じようにして，2乗パラメータ更新の和 (SSPU: sum of squared parameter updates) を次のように定義できます．

$$
SSPU^{t}_{i} = \gamma \times SSPU^{t-1}_{i} +(1 - \gamma) \times (\Delta\beta^{t}_{i})^2
$$

これで次元の問題は解決し，パラメータの更新式は次のように書き換えられます．

$$
\beta^{t+1}_{i} = \beta^{t}_{i} - \frac{\sqrt{SSPU^{t}_{i}+\epsilon}}{\sqrt{SSG^{t}_{i}+\epsilon}} \times \frac{\delta L(X, y, \beta)}{\delta\beta^{t}_{i}}
$$

注目すべきは，この式に学習率が必要ない点です．
しかし，学習率を乗算で与えることが可能です．
したがって，必要なハイパーパラメータは $\gamma$ だけになります．

### RMSprop
先ほどの Adadelta の項では，暗に RMSprop の内容にも触れていました．
RMSprop と Adadelta の違いは次元の問題を解決しているかいないかという点だけです．
そのため，Adadelta の項の最初の式と同じ式で表されます．
したがって，加重平均の重みだけでなく学習率についてもハイパーパラメータとして設定する必要があります．

### Adam (Adaptive Moment Estimation)
この最適化手法も各パラメータの学習率を調節可能なものとなっています．
Adadelta や RMSprop と同様に，直前の勾配の2乗値との加重平均から学習率を決定します．
しかし，Adam では直前の勾配値の加重平均も使います．

$$
SG^{t}_{i} = \gamma' \times SG^{t-1}_{i} + (1 - \gamma') \times \frac{\delta L(X, y, \beta)}{\delta\beta^{t}_{i}}
$$

SGおよびSSGは、数学的にはそれぞれ勾配の第1および第2モーメントを推定することと同じであるため、この手法の名前は「適応的モーメント推定」となっています。
通常，$\gamma$ や $\gamma'$ は 1 に近い値を取ります．
この時，SG や SSG の初期値がほぼ 0 になってしまいます．
これを防ぐため，これらの値はバイアス補正によって次式で再定義します．

$$
SG^{t}_{i} = \frac{SG^{t}_{i}}{1-\gamma'}, SSG^{t}_{i} = \frac{SSG^{t}_{i}}{1-\gamma}
$$

最後に，パラメータの更新は次式のように行います．

$$
\beta^{t+1}_{i} = \beta^{t}_{i} - \frac{\alpha}{\sqrt{SSG^{t}_{i}+\epsilon}} \times SG^{t}_{i}
$$

基本的に，式の一番右側にある勾配が勾配の加重平均に置き換えられています．
注目すべきは，Adam にはベースとなる学習率と2つの加重割合の合計3つのハイパーパラメータが必要となる点です．
Adam は複雑な深層学習モデルの学習において，最近の最適化手法の中で最も成功した手法の一つです．  
それでは，どの最適化手法を使えばよいのでしょうか？
それは場合によります．
スパースなデータを扱うときには，パラメータごとに学習率を更新できるので，適応的な最適化が有利となります．
先にも述べた通り，スパースなデータを扱う場合には，それぞれのパラメータが異なるペースで作用するので，
パラメータごとに学習率を調整できることで，モデルが最適解に到達しやすくなります．
SGD でも最適解に到達することは可能ですが，学習により時間がかかってしまいます．
適応的な最適化手法の中では，学習率の分母が単調に増大し続けて学習率がほとんど 0 になってしまうため，
Adagrad は不利となります．  
RMSprop と Adadelta，Adam は様々な深層学習において非常に近い性能が得られます．
RMSprop はベースの学習率を用いる反面，Adadelta は直前のパラメータの加重平均によって更新を行うということを除き，この2種類はほとんど同じです．
Adam は勾配の1次モーメントの計算が含まれており，バイアス補正も考慮されている点で，上記の二つとは少し異なります．
全体として，Adam は他の条件が同じであれば最適化を行うことができます．
本書の演習では，これらの最適化スケジュールの一部を使用します，
自由に別のものと入れ替えて、以下のような変化を観察してみてください．

- モデルの学習時間と軌跡 (収束)
- 最終的なモデルの性能

これからの章では，これらのアーキテクチャやレイヤ，活性化関数，最適化スケージュールの多くを使って，PyTorch の力を借りながら様々な種類の機械学習の問題を解いていきます．
本章の例では畳み込み層と線形層，最大値プーリング，そしてドロップアウト層を使って CNN を構築します．
活性化関数には，最終層は対数ソフトマックス (Log-Softmax) を用いて，その他の層は ReLU を用いています．
学習率を 0.5 で固定した Adadelta を用いてモデルの最適化を行います．

## PyTorch ライブラリの探索
PyTorch は Torch ライブラリに基づく Python の機械学習ライブラリです．
PyTorch は深層学習ツールとして研究や産業向けのアプリケーションの構築に広く利用されています．
当初，PyTorch は Facebook の機械学習研究所で開発されていました．
PyTorch の競合には Google によって開発された TensorFlow という有名な深層学習ライブラリがあります．
初めのうち，この2つのライブラリには，PyTorch が eager execution であるのに対して，
TensorFlow が graph-based deferred execution であるという違いがありました．
しかし，現在では TensorFlow も eager execution モードを提供しています．

Eager execution は，基本的に数学的な演算がすぐに処理される命令的なプログラミングのモードです．
Deferred execution モードでは，計算処理は行わずに全ての演算を計算グラフにストアしておいて，
後からグラフ全体の評価を行います．
直感的なフローでデバックが容易であり，スキャフォールディングではないコードにできるという点で，
eager execution に分があります．

PyTorch はただの深層学習モデルにとどまりません．
NumPy に似た文法やインターフェイスで GPU による強力な高速化が可能なテンソル計算能力があります．
ところで，テンソルとは何なのでしょうか？
テンソルは計算の単位で，計算を GPU を使って高速化できる点を除いて，NumPy 配列と非常によく似ています．

計算の高速化と動的な計算グラフを構築する機能によって，PyTorch は深層学習の完全なフレームワークを提供します．
さらに，Python 向けに作られていることから，Python のデータサイエンス向けの拡張機能を含む，
様々な Python の特徴を PyTorch に組み入れることが可能です．

本章では，データの読み込みやモデルの構築，モデル学習時の最適化スケジュールの指定に使える PyTorch モジュールの様々な機能を見ていきます．
また，テンソルとは何者で，PyTorch の属性の中でどのように実装されているのかについても見ていきます．

### PyTorch モジュール
[Jupyter Notebook](./section1_pytorch_modules.ipynb) 参照．

### テンソル (Tensor) モジュール
[Jupyter Notebook](./section1_tensor_modules.ipynb) 参照．

### PyTorch を用いたニューラルネットワークの学習
